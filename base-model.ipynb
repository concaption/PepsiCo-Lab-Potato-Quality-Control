{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/concaption/PepsiCo-Lab-Potato-Quality-Control/blob/main/Potato_Starter_Code_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"import numpy as np  \nimport datetime\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.layers import  Flatten, Dense, Dropout\nfrom tensorflow.keras import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt","metadata":{"id":"Yg0adGuaY-80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Global Variables -- \nTRAIN_PATH = '/content/drive/MyDrive/Pepsico RnD Potato Lab Dataset/Train'\nTEST_PATH = '/content/drive/MyDrive/Pepsico RnD Potato Lab Dataset/Test'\nBATCH_SIZE = 32\nCOLOR_MODE = 'rgb'\nTARGET_SIZE = (255, 255)\nGRAY_SCALL = (3,)\nINPUT_SIZE = TARGET_SIZE + GRAY_SCALL\nEPOCHS = 10\nCLASSES = ['Defective','Non-Defective']","metadata":{"id":"0Oh3IXZwaC1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Data Normalization --\ndata_generator = ImageDataGenerator(samplewise_center=True, #making sure that each image has a mean of 0\n                                    samplewise_std_normalization=True, #and standard deviation 1\n                                    horizontal_flip=True, #Randomly flip inputs horizontally\n                                    validation_split=0.3,\n                                    )","metadata":{"id":"VKeMH4hivKoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Data iterators -- \ntrain_data = data_generator.flow_from_directory(directory=TRAIN_PATH,\n                                                target_size=TARGET_SIZE,\n                                                batch_size=BATCH_SIZE,\n                                                class_mode='categorical',\n                                                color_mode=COLOR_MODE,\n                                                subset='training',\n                                                shuffle=True)         \n    \nvalidation_data = data_generator.flow_from_directory(directory=TRAIN_PATH,\n                                                     target_size=TARGET_SIZE,\n                                                     batch_size=BATCH_SIZE,\n                                                     class_mode='categorical',\n                                                     color_mode=COLOR_MODE,\n                                                     subset='validation',\n                                                     shuffle=True)             \n\ntest_data = data_generator.flow_from_directory(directory=TEST_PATH,\n                                               target_size=TARGET_SIZE,\n                                               batch_size=BATCH_SIZE,\n                                               class_mode='categorical',\n                                               color_mode=COLOR_MODE,\n                                               shuffle=True)","metadata":{"id":"qpKwI-gieHGs","outputId":"035bc2b3-7008-4237-ce59-d97c36d55d93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- plot random batch -- \nimages, labels = train_data.next()\nclasses = np.asarray(CLASSES)\n\n_, axs = plt.subplots(4, 8, figsize=(12,12))\naxs = axs.flatten()\nfor img, label, ax in zip(images, labels, axs):\n    ax.imshow(img)\n    ax.axis('off')\n    label = label.astype(int)\n    ax.set_title(classes[label == 1])\nplt.show()","metadata":{"id":"O8T1o3LFndP_","outputId":"286cae72-ef0c-4fa1-a16b-cc55f331708c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_model():\n  vgg19_model = VGG19(weights='imagenet',include_top=False,input_shape=INPUT_SIZE)\n  vgg19_model.trainable = False\n  flatten =Flatten()(vgg19_model.layers[-1].output)\n  fc1 = Dense(units=4096, activation ='relu')(flatten)\n  dropout = Dropout(0.2)(fc1)\n  fc2 = Dense(units=1024,activation='relu')(dropout)\n  output = Dense(2, activation='softmax')(fc2)\n  model = Model(inputs = vgg19_model.input, outputs=output)\n  model.summary()\n  return model","metadata":{"id":"tqj-MHY92Mtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = my_model()","metadata":{"id":"gpCiq_PhA7cq","outputId":"4f9333ab-4f07-4cce-b140-a6d430a5af30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True, show_dtype=False,\n    show_layer_names=True, rankdir='T', expand_nested=False, dpi=96\n)","metadata":{"id":"xKWixX7OWKT3","outputId":"0a0860e7-bd69-4376-9a4a-7fbb07deced8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Define optimizer and loss --\nopt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nloss = tf.keras.losses.CategoricalCrossentropy()","metadata":{"id":"uS8JGgTqDFBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Compile model --\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])","metadata":{"id":"J49bvHSBDLdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # -- Callbacks --\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath='my_model.h5', \n                                                    monitor='accuracy', verbose=1, \n                                                    save_best_only=True, \n                                                    save_weights_only=False, \n                                                    mode='auto', \n                                                    save_freq='epoch')\n    \nearlystoping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', \n                                                    min_delta=0, \n                                                    patience=5,  #Number of epochs with no improvement after which training will be stopped.\n                                                    verbose=1, \n                                                    mode='auto')\n    \nlog_dir = './logs/fit/' + datetime.datetime.now().strftime('%m.%d.%Y--%H-%M-%S')\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n                                                 histogram_freq=1, \n                                                 write_graph=True,\n                                                 write_images=False, \n                                                 update_freq='epoch')","metadata":{"id":"aUJ4I3fNDNk_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Train model --\nhistory = model.fit(x=train_data, \n                        epochs=EPOCHS, \n                        steps_per_epoch=len(train_data), \n                        verbose=1, \n                        validation_data=validation_data, \n                        validation_steps=1, \n                        callbacks=[checkpoint, earlystoping, tensorboard])\n    \n# -- Save model -- \nmodel.save('my_model.h5')","metadata":{"id":"txuAFiFeDUWQ","outputId":"2df849ad-2a10-4559-ae89-7df1115abbe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def learning_curves(history):\n    '''plot learning curves'''\n    \n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    plt.figure(figsize=(10, 8))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Loss - Cross Entropy')\n    plt.xlabel('epoch')\n    plt.ylim([0,1.6])\n    plt.title('Training and Validation Loss')\n    \n    plt.show()","metadata":{"id":"T6zfLD1hDWSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Plot learning curves -- \nlearning_curves(history)","metadata":{"id":"POedw4t_Daup","outputId":"ffc4be09-c38c-49ad-c92a-c37a4c6ea18b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- Evaluate the model on the test data -- \nloss, accuracy = model.evaluate(x=test_data)\nprint(\"test loss: \", loss, \", test acc: \" , 100*accuracy, \"%\")","metadata":{"id":"teTuRZ3WDcZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def defective_or_not(img_path):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(255,255,3))\n    img = np.asarray(img)\n    img = np.expand_dims(img, axis=0)\n    model = tf.keras.models.load_model('my_model.h5')\n    output = model.predict(img)\n    print(classes[output[0]==1])","metadata":{"id":"5ow6LebBDd-W"},"execution_count":null,"outputs":[]}]}